# -*- coding: utf-8 -*-
"""Demo Script.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iaK9MOjOf9GV7AF_hjzaQjZbQGFgYRLR
"""

!pip install pandas numpy scikit-learn spacy nltk bs4 matplotlib seaborn wordcloud

# ============================
# 1. Import Libraries
# ============================
import pandas as pd
import numpy as np
import pickle
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout

# ============================
# 2. Load Data
# ============================
train_df = pd.read_csv("/content/drive/MyDrive/train_clean.csv")
test_df  = pd.read_csv("/content/drive/MyDrive/test_clean.csv")

X_train_text = train_df['review_clean']
X_test_text  = test_df['review_clean']
y_train = train_df['sentiment'].astype(int).values
y_test  = test_df['sentiment'].astype(int).values

# ============================
# 3. Tokenization & Padding
# ============================
MAX_VOCAB = 30000
MAX_LEN = 200

tokenizer = Tokenizer(num_words=MAX_VOCAB)
tokenizer.fit_on_texts(X_train_text)

X_train_seq = tokenizer.texts_to_sequences(X_train_text)
X_test_seq  = tokenizer.texts_to_sequences(X_test_text)

X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN)
X_test_pad  = pad_sequences(X_test_seq, maxlen=MAX_LEN)

# ============================
# 4. Build LSTM Model
# ============================
model = Sequential([
    Embedding(input_dim=MAX_VOCAB, output_dim=128, input_length=MAX_LEN),
    LSTM(64, return_sequences=False, dropout=0.4, recurrent_dropout=0.4),
    Dropout(0.5),
    Dense(32, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# ============================
# 5. Train Model
# ============================
early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)
model.fit(
    X_train_pad, y_train,
    validation_split=0.2,
    epochs=5,
    batch_size=64,
    callbacks=[early_stop],
    verbose=1
)

# ============================
# 6. Save Model & Tokenizer
# ============================
model.save("lstm_sentiment_model.h5")
with open("tokenizer.pkl", "wb") as f:
    pickle.dump(tokenizer, f)

print("âœ… Model and Tokenizer Saved!")

import pickle
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences

# ============================
# 1. Load Model & Tokenizer
# ============================
model = load_model("/content/lstm_sentiment_model.h5")
with open("/content/tokenizer.pkl", "rb") as f:
    tokenizer = pickle.load(f)

MAX_LEN = 200  # Must match training

# ============================
# 2. Prediction Function
# ============================
def predict_sentiment(text):
    seq = tokenizer.texts_to_sequences([text])
    pad_seq = pad_sequences(seq, maxlen=MAX_LEN)
    prob = model.predict(pad_seq)[0][0]
    sentiment = "Positive ðŸ˜€" if prob > 0.5 else "Negative ðŸ˜ "
    return sentiment, prob

# ============================
# 3. Interactive Demo Loop
# ============================
print("=== Sentiment Analysis Demo (LSTM) ===")
while True:
    user_input = input("\nEnter a review (or 'quit' to exit): ")
    if user_input.lower() == 'quit':
        print("ðŸ‘‹ Goodbye!")
        break
    sentiment, prob = predict_sentiment(user_input)
    print(f"Predicted Sentiment: {sentiment}  |  Confidence: {prob:.4f}")