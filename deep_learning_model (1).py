# -*- coding: utf-8 -*-
"""Deep Learning model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I5DURfuH77Cjhh2ZCKqtTVdM-KmrF3XZ
"""

!pip install pandas numpy scikit-learn spacy nltk bs4 matplotlib seaborn wordcloud

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout

# =====================
# 1. Load Cleaned Data
# =====================
train_df = pd.read_csv("/content/drive/MyDrive/IMDB_Project/train_clean.csv")
test_df  = pd.read_csv("/content/drive/MyDrive/IMDB_Project/test_clean.csv")

# =====================
# 2. Prepare Text and Labels
# =====================
X_train_text = train_df['review_clean']
X_test_text  = test_df['review_clean']
y_train = train_df['sentiment'].astype(int).values
y_test  = test_df['sentiment'].astype(int).values

# =====================
# 3. Tokenize & Pad Sequences
# =====================
MAX_VOCAB = 30000  # vocabulary size
MAX_LEN = 200      # sequence length

tokenizer = Tokenizer(num_words=MAX_VOCAB)
tokenizer.fit_on_texts(X_train_text)

X_train_seq = tokenizer.texts_to_sequences(X_train_text)
X_test_seq  = tokenizer.texts_to_sequences(X_test_text)

X_train_pad = pad_sequences(X_train_seq, maxlen=MAX_LEN)
X_test_pad  = pad_sequences(X_test_seq, maxlen=MAX_LEN)

# =====================
# 4. Build Optimized LSTM Model
# =====================
model = Sequential([
    Embedding(input_dim=MAX_VOCAB, output_dim=128, input_length=MAX_LEN),
    LSTM(64, return_sequences=False, dropout=0.4, recurrent_dropout=0.4),  # reduced units & higher dropout
    Dropout(0.5),
    Dense(32, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Build model explicitly for summary
model.build(input_shape=(None, MAX_LEN))
model.summary()

# =====================
# 5. Early Stopping Callback
# =====================
early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)

# =====================
# 6. Train Model
# =====================
history = model.fit(
    X_train_pad, y_train,
    validation_split=0.2,
    epochs=10,
    batch_size=64,
    callbacks=[early_stop],
    verbose=1
)

# =====================
# 7. Evaluate Model
# =====================
y_pred_prob = model.predict(X_test_pad)
y_pred = (y_pred_prob > 0.5).astype(int)

acc  = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec  = recall_score(y_test, y_pred)
f1   = f1_score(y_test, y_pred)

print("\nLSTM Performance:")
print(f"Accuracy:  {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall:    {rec:.4f}")
print(f"F1 Score:  {f1:.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred, digits=4))

# =====================
# 8. Confusion Matrix
# =====================
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative','Positive'], yticklabels=['Negative','Positive'])
plt.title('Confusion Matrix - LSTM')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()

# =====================
# 9. Accuracy & Loss Plots
# =====================
plt.figure(figsize=(8,4))
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('LSTM Model Accuracy Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize=(8,4))
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('LSTM Model Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

from tensorflow.keras.layers import Bidirectional

# =====================
# 4. Build BiLSTM Model
# =====================
bilstm_model = Sequential([
    Embedding(input_dim=MAX_VOCAB, output_dim=128, input_length=MAX_LEN),
    Bidirectional(LSTM(64, return_sequences=False, dropout=0.4, recurrent_dropout=0.4)),
    Dropout(0.5),
    Dense(32, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])

bilstm_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

bilstm_model.build(input_shape=(None, MAX_LEN))
bilstm_model.summary()

# =====================
# 6. Train Model (BiLSTM)
# =====================
early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)

bilstm_history = bilstm_model.fit(
    X_train_pad, y_train,
    validation_split=0.2,
    epochs=10,
    batch_size=64,
    callbacks=[early_stop],
    verbose=1
)

# =====================
# 7. Evaluate Model
# =====================
y_pred_prob = bilstm_model.predict(X_test_pad)
y_pred = (y_pred_prob > 0.5).astype(int)

acc  = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec  = recall_score(y_test, y_pred)
f1   = f1_score(y_test, y_pred)

print("\nBiLSTM Performance:")
print(f"Accuracy:  {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall:    {rec:.4f}")
print(f"F1 Score:  {f1:.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred, digits=4))

# =====================
# 8. Confusion Matrix
# =====================
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative','Positive'], yticklabels=['Negative','Positive'])
plt.title('Confusion Matrix - BiLSTM')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()

# =====================
# 9. Accuracy & Loss Plots
# =====================
plt.figure(figsize=(8,4))
plt.plot(bilstm_history.history['accuracy'], label='Train Accuracy')
plt.plot(bilstm_history.history['val_accuracy'], label='Validation Accuracy')
plt.title('BiLSTM Model Accuracy Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize=(8,4))
plt.plot(bilstm_history.history['loss'], label='Train Loss')
plt.plot(bilstm_history.history['val_loss'], label='Validation Loss')
plt.title('BiLSTM Model Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D

# =====================
# 4. Build CNN Model
# =====================
cnn_model = Sequential([
    Embedding(input_dim=MAX_VOCAB, output_dim=128, input_length=MAX_LEN),
    Conv1D(filters=128, kernel_size=5, activation='relu'),
    GlobalMaxPooling1D(),
    Dense(64, activation='relu'),
    Dropout(0.5),
    Dense(32, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])

cnn_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

cnn_model.build(input_shape=(None, MAX_LEN))
cnn_model.summary()

# =====================
# 6. Train Model (CNN)
# =====================
early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)

cnn_history = cnn_model.fit(
    X_train_pad, y_train,
    validation_split=0.2,
    epochs=10,
    batch_size=64,
    callbacks=[early_stop],
    verbose=1
)

# =====================
# 7. Evaluate Model
# =====================
y_pred_prob = cnn_model.predict(X_test_pad)
y_pred = (y_pred_prob > 0.5).astype(int)

acc  = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec  = recall_score(y_test, y_pred)
f1   = f1_score(y_test, y_pred)

print("\nCNN Performance:")
print(f"Accuracy:  {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall:    {rec:.4f}")
print(f"F1 Score:  {f1:.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred, digits=4))

# =====================
# 8. Confusion Matrix
# =====================
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative','Positive'], yticklabels=['Negative','Positive'])
plt.title('Confusion Matrix - CNN')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()

# =====================
# 9. Accuracy & Loss Plots
# =====================
plt.figure(figsize=(8,4))
plt.plot(cnn_history.history['accuracy'], label='Train Accuracy')
plt.plot(cnn_history.history['val_accuracy'], label='Validation Accuracy')
plt.title('CNN Model Accuracy Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize=(8,4))
plt.plot(cnn_history.history['loss'], label='Train Loss')
plt.plot(cnn_history.history['val_loss'], label='Validation Loss')
plt.title('CNN Model Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

from tensorflow.keras.layers import GRU

# =====================
# 4. Build GRU Model
# =====================
gru_model = Sequential([
    Embedding(input_dim=MAX_VOCAB, output_dim=128, input_length=MAX_LEN),
    GRU(64, return_sequences=False, dropout=0.4, recurrent_dropout=0.4),
    Dropout(0.5),
    Dense(32, activation='relu'),
    Dropout(0.3),
    Dense(1, activation='sigmoid')
])

gru_model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

gru_model.build(input_shape=(None, MAX_LEN))
gru_model.summary()

# =====================
# 6. Train Model (GRU)
# =====================
early_stop = EarlyStopping(monitor='val_loss', patience=2, restore_best_weights=True)

gru_history = gru_model.fit(
    X_train_pad, y_train,
    validation_split=0.2,
    epochs=10,
    batch_size=64,
    callbacks=[early_stop],
    verbose=1
)

# =====================
# 7. Evaluate Model
# =====================
y_pred_prob = gru_model.predict(X_test_pad)
y_pred = (y_pred_prob > 0.5).astype(int)

acc  = accuracy_score(y_test, y_pred)
prec = precision_score(y_test, y_pred)
rec  = recall_score(y_test, y_pred)
f1   = f1_score(y_test, y_pred)

print("\nGRU Performance:")
print(f"Accuracy:  {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall:    {rec:.4f}")
print(f"F1 Score:  {f1:.4f}")
print("\nClassification Report:")
print(classification_report(y_test, y_pred, digits=4))

# =====================
# 8. Confusion Matrix
# =====================
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Negative','Positive'], yticklabels=['Negative','Positive'])
plt.title('Confusion Matrix - GRU')
plt.ylabel('True Label')
plt.xlabel('Predicted Label')
plt.show()

# =====================
# 9. Accuracy & Loss Plots
# =====================
plt.figure(figsize=(8,4))
plt.plot(gru_history.history['accuracy'], label='Train Accuracy')
plt.plot(gru_history.history['val_accuracy'], label='Validation Accuracy')
plt.title('GRU Model Accuracy Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize=(8,4))
plt.plot(gru_history.history['loss'], label='Train Loss')
plt.plot(gru_history.history['val_loss'], label='Validation Loss')
plt.title('GRU Model Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()